---
title: "Take Home Exercise 2"
author: "Oh Jia Wen"
date: 12/11/2023
date-modified: "last-modified"
execute: 
  echo: true
  eval: true
  warning: false
---

# 1. Overview

As city-wide urban infrastructures such as public bus, mass rapid transits, public utilities and roads become digital, data sets obtained could be used as a framework for tracking movement patterns through space and time.

## 1.1 The Task

This exercise aims to reveals the urban mobility patterns of public bus transit through geospatial data science and analysis (GDSA), and spatial interaction models. ars The original data was downloaded on 18th November 2023 from **LTA DataMall** under Section 2.6 - *Passenger Volume by Origin Destination Bus Stops*. It records the number of trips by weekdays and weekends from origin to destination bus stops. For the purpose of this exercise, we will be using the **August** data - origin_destination_bus_202308.csv for **Weekday Morning Peak (6-9am)**.

# 2. Data preparation

## 2.1 Install R packages

The code chunk below uses `pacman:: p_load()` to load and install the following libraries:

-   **`DT`**

-   **`ggpubr`**

-   **`performance`** : Used for computing measures to access model quality that are not directly provided by R's 'base' or 'stats' packages.

-   **`reshape2`** : Used to restructure and aggregate data

-   **`sf`** : Used for geospatial data handling

-   **`spdep`** : Used to create spatial weights matrix objects

-   **`sfdep`** : Used to integrate with `'sf'` objects and the `'tidyverse'`

-   **`tidyverse`**: A collection of R packages use in everyday data analyses. It is able to support data science, data wrangling, and analysis

-   **`tmap`** : Used for thematic mapping where spatial data distributions are visualized.

```{r}
pacman::p_load(DT, ggpubr, performance, reshape2, sp, 
               sf, spdep, sfdep, stplanr, tidyverse, tmap)
```

## 2.2 Import and Load Dataset

For the purpose of this exercise, we will be using three data sets.

-   `origin_destination_bus_202308.csv` : A csv file containing information about all the bus stops currently being serviced by bus, which includes bus stop identifier, and location coordinates.

-   *`Bus Stop Location`* . A geospatial file retrieved from LTA DataMall

-   `Master Plan 2019 Subzone Boundary` : A polygon feature layer in ESRI shapefile format.

### 2.2.1 Importing Aspatial data

First, we will import the [Passenger Volume by Origin Destination Bus Stops]{.underline} data set for **August** by using `readr::read_csv()` and store it in variable **odbus**. Also, we will be using `glimpse()` report to reveal the data type of each field.

*Point to note: `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` are in `<chr>` format.*

```{r}
#| code-fold: true
odbus <- read_csv("data/aspatial/origin_destination_bus_202308.csv")
glimpse(odbus)

```

### 2.2.2 Importing Geospatial data

Thereafter, we will import the [Bus Stops]{.underline} [Location]{.underline} and [Master Plan 2019 Subzone Boundary]{.underline}.

#### 2.2.2.1 Import Bus Stop data

We will be using `sf::st_read()` to import and `sf::st_transform()` to ensure that the projected coordinate system is in the right format before storing in variable **busstop**.

```{r}
busstop <- st_read(dsn = "data/geospatial",layer = "BusStop") %>%
    st_transform(crs = 3414)
```

Also, we will be using `glimpse()` report to reveal the data type of each field.

```{r}
glimpse(busstop)
```

#### 2.2.2.2 Import MPSZ data

Similarily, we used `sf::st_read()` to import and `sf::st_transform()` to ensure that the projected coordinate system is in the right format before storing in variable **mpsz**.

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

We will be using `glimpse()` report to reveal the data type of each field

```{r}
glimpse(mpsz)
```

## 2.3 Data Wrangling

Looking at section 2.2.1, we noticed a few problem:

-   *`ORIGIN_PT_CODE`* : is in `<chr>` format

-   *`DESTINATION_PT_CODE`* : is in `<chr>` format

We will be using `dplyr::mutate()` to convert the `<chr>` data type to `<fct>` and store it in a new variable **odbus_new**.

```{r}
#| code-fold: true
odbus_new <- odbus %>%
 mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
        DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))

glimpse(odbus_new)
```

Additionally, we confirmed that there are no missing values in the **odbus_new data set.**

```{r}
any(is.na(odbus_new))
```

### 2.3.1 Data Extraction

#### 2.3.1.1 Analytical Hexagon

To get a better representative of the traffic analysis zone (TAZ), we will create an analytical hexagon grid. With reference from [Urban Data Palette](https://urbandatapalette.com/post/2021-08-tessellation-sf/), we will used the code chunk below and execute in the following sequence:

-   `st_make_grid()` : create a regular grid of spatial polygons. To reflect the distance from centre of the hexagon to its edge (375m), we will revise the cell width and height to c(750,750) to reflect the full diameter.

-   `st_sf()` : convert the honeycomb grid object to an sf object. We used `[mpsz$geometry]` to identify hexagon grid that intersects.

-   `rowid_to_column()` : add rowid as a column. Values in this column are the row IDs of each grid cell.

-   `lengths(st_intersects())` : assigns the number of bus stops for each cell to a new column named ***'bus_stops'*** in hex_grid. `st_intersects()` checks for intersections between two spatial objects.

-   `as.factor()` : to coverts the ***'grid_id'*** column from`<int>` to `<fct>`

-   `tmap_options(check.and.fix = TRUE)` : forcing the polygons to be closed

-   `tm_shape()` + `tm_polygons()` : plot the honeycomb grid cells as polygons

```{r}
#| code-fold: true
#create regular grid 
honeycomb = st_make_grid(mpsz,c(750,750),what = "polygons", square = FALSE)

#convert our honeycomb grid into an sf object and add rowid
hex_grid <- st_sf(honeycomb[mpsz$geometry]) %>%
  rowid_to_column('grid_id')

#add no. of bus stops into hex_grid data frame
hex_grid$bus_stops = lengths(st_intersects(hex_grid, busstop))

#convert grid_id from int to fct 
hex_grid$grid_id <- as.factor(hex_grid$grid_id)

#plotting of Master Plan 2019 Subzone Boundary in analytical hexagon
tmap_options(check.and.fix = TRUE) #force close polygon 
tm_shape(hex_grid) +
  tm_polygons() +
 tm_layout(main.title = "Analytical Hexagon Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_borders(alpha = 0.3)
```

#### 2.3.1.2 Commuting Flow

Through the previous [exercise](https://isss624-ohjiawen.netlify.app/take-home_ex/take-home_ex01/take-home_ex01) in Take Home Exercise 1, we were able to have a better understanding of the commuting flows. We noticed that the Peak hour period for Weekdays were generally higher as compared to Weekends, which we believe, could bring us more insights. As such, we will focus on our commuting flow at **Weekday Morning Peak (6-9am)**.

The code is extracted in the following sequence:

-   `filter()` is used to extract subset of data for the `DAY_TYPE` and `TIME_PER_HOUR`

-   `between()` is used to express a range condition for the `TIME_PER_HOUR`

-   `group_by()` is used to grouped data. In our code, we will be grouping it based on `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE`

-   `summarise()` is used to sum the total number of trips

-   `arrange(desc())` is used to sort in descending order

-   `ungroup()` is used to end a definition, often use with `group_by()`

Thereafter, we will save a copy of the output in rds format called **wkd6_9**.

```{r}
#| code-fold: true
#| eval: false
wkd6_9 <- odbus_new %>%
  filter(DAY_TYPE == "WEEKDAY",
         between(TIME_PER_HOUR, 6, 9)) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  arrange(desc(TRIPS)) %>%
  ungroup()

#save a copy in RDS format 
write_rds(wkd6_9, "data/rds/wkd6_9.rds")
```

As we are not executing the code chunk above, we will load **wkd6_9** into the R environment.

```{r}
wkd6_9 <- read_rds("data/rds/wkd6_9.rds")
```

#### 2.3.1.3 Visualing bus stops in hexagon grid

After importing the bus stop location in Section 2.2.2.1, we will create a visualization to observe the number of bus stops in each hexagon grid ***(grid_id)***. To get a better contrast, we incorporated **RcolorBrewer** into our code by adding a purple palette.

```{r}
#| code-fold: true
#plot bus stop counts on hexagon layer 
  tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
  tm_layout(main.title = "Number of Bus Stops in Hexagon Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
  tm_borders(alpha =0.3)
  
```

On average, we could see that most of the hexagon grids contain 6 to 10 bus stops with lesser dark colors hexagon grid as the data value increases. With the use of hexagon, we are able to identify the nearest neighbor by their common boundaries, infer that there might be some degree of influence as the colors does not differ significantly.

::: {.callout-note collapse="true" appearance="simple"}
## More about Tmap Packages

-   `tm_shape() + Base Layer` : Main plotting method. Used to specify a shape object.

```         
-   Base Layer includes:

    -   `tm_polygons()`  - Create a polygon later (with borders)

    -   `tm_symbols()`  - Create a layer of symbols

    -    `tm_lines()`  - Create a layer of lines

    -    `tm_raster()`  - Create a raster layer

    -    `tm_text()`  - Create a layer of text labels

    -   `tm_basemap()`  - Create a layer of basemap tiles

    -   `tm_tiles()`  - Create a layer of overlay tiles
```

-   Aesthetics derived layers, which includes:

```         
-   `tm_fill()` :  - Create a polygon layer (without borders)

-   `tm_borders()` :  - Create a polygon borders

-   `tm_bubbles()` :  - Create a layer of bubbles

-   `tm_squares()` :  - Create a layer of squares

-   `tm_dots()` :  - Create a layer of dots

-   `tm_markers()` :  - Create a layer of markers

-   `tm_iso()` :  - Create a layer of iso/contour lines

-   `tm_rgb()` :  - Create a raster layer of an image
```

-   Facets

-   Attributes

-   Layout elements

-   Tmap_mode

For more information, click [here](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/tmap/tmap.pdf).
:::

## 2.4 Data Generation

### 2.4.1 Combining Busstop and mpsz

After extracting the commuting flow for **Weekday Morning Peak**, we will combine it into **busstop** sf data frame. To ensure distinct flows, we will use the `unique()` function into our code.

```{r}
busstop_mpsz <- st_intersection(busstop, hex_grid) %>%
    select(BUS_STOP_N, grid_id) %>%
  st_drop_geometry()
```

Thereafter, we will perform the `left_join()` function twice. Firstly, we will join it based on `ORIGIN_PT_CODE` , where we will rename some of the columns and retain unique records. Then, we will join it based on `DESTINATION_PT_CODE` which we have renamed it as `DESTIN_BS` earlier.

After joining the data, we used `drop_na()` to removes any rows with missing values, and to group the data by origin and destination ID while computing the sum of trips for each origin-destination pair, saved under **MORNING_PEAK**. Lastly, a copy of the output - **od_data** was saved in RDS format.

```{r}
#| code-fold: true
#| eval: false
#left_join based on Origin_PT_Code
od_data <- left_join(wkd6_9 , busstop_mpsz,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_ID = grid_id,
         DESTIN_BS = DESTINATION_PT_CODE) %>%
  unique() %>%
  ungroup()

#left_join based on Destination_PT_code(rename as DESTIN_BS)
od_data <- left_join(od_data , busstop_mpsz,
            by = c("DESTIN_BS" = "BUS_STOP_N")) %>%
  unique() %>%
  ungroup()

# compute the morning peak trips between origin and destination grid
od_data <- od_data %>%
  rename(DESTIN_ID = grid_id) %>%
  drop_na() %>%
  group_by(ORIGIN_ID, DESTIN_ID) %>%
  summarise(MORNING_PEAK = sum(TRIPS)) %>%
  ungroup()

#save a copy in RDS format
write_rds(od_data, "data/rds/od_data.rds")
```

We will load **od_data** into the R environment.

```{r}
od_data <- read_rds("data/rds/od_data.rds")
```

### 2.4.2 Visualizing Spatial Interaction

With the joined data - **od_data**, we will prepare a desire line by using the **stplanr** package by removing the intra-zonal flows, creating the desire lines, and to visualize the desire lines.

#### 2.4.2.1 Remove Intra-zonal flows

Intra-zonal flows refer to trips that take place within the same zone or area. In our context, it refers to the passenger commuting within the hexagon grid of 750m. As we will not be plotting the intra-zonal flows, the following code chunk below are used to only keep rows where the `Origin_ID` is not equal to the `DESTIN_ID`.

```{r}
od_data1 <- od_data[od_data$ORIGIN_ID!=od_data$DESTIN_ID,]
```

#### 2.4.2.2 Create desire lines

Using the `stplanr::od2line()` function, we will create a line object where the ***flow*** refers to the data containing origin and destination information, ***zones*** refers to spatial data presenting the zone geometries, while ***zone_code*** refers to the specific column in our *zones* data that could identify the zone. In our case, it will be the `grid_id`. The output will be saved as **flowLine**.

```{r}
flowLine <- od2line(flow = od_data1, 
                    zones = hex_grid,
                    zone_code = "grid_id")
```

#### 2.4.2.3 Visualizing desire lines

![](images/image1.png)

After creating the **flowLine** , we will visualize the resulting desire lines. Notably, quantiles provide a detailed picture of the how the `MORNING_PEAK` values are spread across. It allows us to identify if the flow is concentrated or evenly distributed across the network.

```{r}
quantile(flowLine$MORNING_PEAK,
         probs = c(0.8, 0.9, 0.95, 0.99, 1))
```

From the results above, we will use **tmap** package to plot our visualization for the 80th percentile, 95% percentile, and 99% percentile. We began by plotting the background map `tm_shape(hex_grid) + tm_polygons()`, and filtering the **flowLine** to include flows based on the quantiles identified earlier. This allows us to focus on the most significant flow movements.

Afterwards, we used `tm_shape() + tm_lines()` to add flow lines into our visualization. In addition, we reduce the transparency of the lines to get a clear view of the flow. Low `alpha` results in transparency.

::: panel-tabset
# Top 20%

```{r}
#| code-fold: true
tm_shape(hex_grid) +
  tm_polygons(col = 'grey95') +
flowLine %>%
    filter(MORNING_PEAK >= 255) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",  #set line width 
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of the top 20% ",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE)

```

# Top 5%

```{r}
#| code-fold: true
tm_shape(hex_grid) +
  tm_polygons(col = 'grey95') +
flowLine %>%
    filter(MORNING_PEAK >= 1479) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of the top 5% ",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE)

```

# Top 1%

```{r}
#| code-fold: true
tm_shape(hex_grid) +
  tm_polygons(col = 'grey95') +
flowLine %>%
    filter(MORNING_PEAK >= 6220) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of the top 1% ",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE)

```
:::

Based on the commuting flow of the **80th percentile**, we could observe various clusters within the region, and commuters spend more travelling from origin to destination. From the thickness of the line, we could infer that the commuters travels the same route frequently - from the North region to the East region, and from the West region to the Central Region. It is interesting to note that commuters like myself do not travel from the West region to the East region frequently.

Looking at the **95th percentile** , the flow movement decreased significantly. The commuting flow around the central region has dropped. Moreover, we are able to observe the flow from the North region to the East region, while we noticed that there are significant flow from the North region to the North-East region.

Narrowing it down to the top 1% (**99% percentile**), we are able identify that the longest commuting distance is from the North region to the North-East region. Also, commuters tend to travel within their region, travelling a short distance from origin to destination.

# 3. Data Integration

After looking at the commuting flows, we are interested to know why people traveled for a particular distance, and the purpose of their trip. Could it be for work? Could it be for school? As such, we will be importing eight collected data - Business, Entertainment, Food & Beverages, Financial Service, Leisure & Recreation, Retails, HDB and School as part of our mobility study.

## 3.1 Importing Collected Data

Similar to section 2.2, we used `sf::st_read()` to import and `sf::st_transform()` to ensure that the projected coordinate system is in the right format before storing it. With the exception of HDB and School data where we used `readr::read_csv()` to import and `sf::st_as_sf()` to converting an aspatial data into sf tibble data.frame.

Before we determine which exploratory variables are attractive or propulsive to a commuter, we plot a map based on the variables. We created a **3-layers** plot by looking at :

-   **Layer 1**: Our base layer `(hex_grid)` . Hexagon grid with bus_stop density.

-   **Layer 2:** Commuting flow movement. Identified based on [purple lines.]{.underline}

-   **Layer 3:** Variables' location. Identified based on [blue dots.]{.underline}

By looking at the intersections between these layers, and our prior knowledge, we could better assemble the variables.

::: panel-tabset
# Business

```{r}
#| code-fold: true
business_sf <- st_read(dsn = "data/geospatial",
                      layer = "Business") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
  flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(business_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Business Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# Entertainment

```{r}
#| code-fold: true
entertn_sf <- st_read(dsn = "data/geospatial",
                      layer = "entertn") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(entertn_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Entertainment Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# F&B

```{r}
#| code-fold: true
fnb_sf <- st_read(dsn = "data/geospatial",
                      layer = "F&B") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(fnb_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Food and Beverages Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# Financial Service

```{r}
#| code-fold: true
finserv_sf <- st_read(dsn = "data/geospatial",
                      layer = "FinServ") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(finserv_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Financial Service Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# HDB

note that some residential area have commercial etc, would be a mixed development but we will include it in our analysis.

```{r}
#| code-fold: true

#import hdb.csv and tidy data 
hdb <- read_csv("data/aspatial/hdb.csv") %>%
  rename(latitude = "lat",
         longitude = "lng") %>%
  filter(residential == "Y") 

#converting an aspatial data into sf tibble data.frame
hdb_sf <- st_as_sf(hdb, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)

#identify intersection between hdb and hex_grid to compute total_dwelling units 
hdb_sf_sum <- st_intersection(hdb_sf, hex_grid) %>%
  group_by(grid_id) %>%
  summarise(
    HDB_SUM = sum(total_dwelling_units)
  ) %>%
  ungroup() 

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(hdb_sf_sum) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "HDB Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# Leisure & Recreation

```{r}
#| code-fold: true
lnr_sf <- st_read(dsn = "data/geospatial",
                      layer = "Liesure&Recreation") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(lnr_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Leisure and Recreation Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 
```

# Retail

```{r}
#| code-fold: true
retail_sf <- st_read(dsn = "data/geospatial",
                      layer = "Retails") %>%
      st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(retail_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "Retail Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 

```

# School

```{r}
#| code-fold: true

#import school.csv into R environment 
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude = 'results.LATITUDE', longitude ='results.LONGITUDE') %>%
  select(postal_code, school_name, latitude, longitude)

#convert an aspatial data into sf tibble data frame
schools_sf <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)

#plotting point symbol map with analytical hexagon grid
tmap_options(check.and.fix = TRUE)
tm_shape(hex_grid) +
  tm_fill(col = "bus_stops",palette = "Purples") +
tm_borders(alpha = 0.3) +
    flowLine %>%
    filter(MORNING_PEAK >= 1000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           col = "purple",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.8) +
    tm_layout(main.title = "Commuting flow of \n passengers with >1,000 trips",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
tm_shape(schools_sf) +
  tm_dots(col = "lightblue") +
  tm_layout(main.title = "School Spot",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 

```
:::

As we are focusing on the Weekday Morning Peak Period from 0600 to 0900, based on my own understanding, I would personally eliminate the Entertainment variables, and Leisure & Recreation as I believe that the operating hours then to begin near noon. However, if we look at the maps above, we could see that the Entertainment spot is clustered around the central region with a handful around Singapore. Thus, we will exclude **Entertainment** for further analysis.

Looking into the Leisure & Recreation tab, we noticed that there are more spots in this field than Entertainment. By looking at the data set we noticed that there are multiple gyms and recreation centres. However, both companies does not operate around the same time, which would affect the accuracy of the data. Upon a closer look on the data, we observed that there are handful of spot that falls besides the grid that contains a bus stop, making it seems inaccessible to commuters. Therefore, we will exclude **Leisure and Recreation** for further analysis.

While looking at the other variables, the Food & Beverage (F&B) spot and School spot caught my eyes. At first glance, I assumed the F&B includes stalls in Hawker Centres, or Coffee Shop as I was expecting a wide amount of network, knowing how Singaporean loves food and would queue for food. Afer a close examination at the data, it is based on fine dining, bar , and pub. Thus, we could safely assume that the operating hours begins from 11.00am. As such, we will exclude **Food & Beverage** from further analysis.

Moreover, it is interesting to note that the School spot intersects with multiple commuting flows with darker bus stop density and thick line width. Moving forward, we will focus on Business, Financial Services, HDB, Retail, and school as our propulsive and attractiveness variables.

### 3.2 Compute point-in-polygon count

After confirming the five variables, we will count the number of spot located inside the grid_id by performing a point-in-polygon count. We will used the `length()` of **Base** package and `st_intersects()` of **sf** package. Thereafter, we will compute and display the summary statistics of computed variables using `summary()`.

::: panel-tabset
# Business

## Count number of Businesses in Hexagon Grid

```{r}
hex_grid$`BUSINESS_COUNT`<- lengths(st_intersects(hex_grid, business_sf))
```

## Summary Statistics

```{r}
summary(hex_grid$BUSINESS_COUNT)
```

# Financial Service

## Count number of Financial Services in Hexagon Grid

```{r}
hex_grid$`finserv_COUNT`<- lengths(st_intersects(hex_grid, finserv_sf))
```

## Summary Statistics

```{r}
summary(hex_grid$finserv_COUNT)
```

# HDB

## Count number of HDB in Hexagon Grid

```{r}
#| code-fold: true
hdb_sf_units <- st_intersection(hdb_sf, hex_grid) %>%
  st_drop_geometry() %>%
  group_by(grid_id) %>%
  summarise(
    HDB_SUM = sum(total_dwelling_units)
  ) %>%
  ungroup() 

hex_grid <- left_join(hex_grid, hdb_sf_units, by = "grid_id") 
hex_grid$HDB_SUM <- replace_na(hex_grid$HDB_SUM, 0)
  
```

## Summary Statistics

```{r}
summary(hex_grid$HDB_SUM)
```

# Retail

## Count number of Retails in Hexagon Grid

```{r}
hex_grid$`retail_COUNT`<- lengths(st_intersects(hex_grid, retail_sf))
```

## Summary Statistics

```{r}
summary(hex_grid$retail_COUNT)
```

# School

## Count number of School in Hexagon Grid

```{r}
hex_grid$`school_COUNT`<- lengths(st_intersects(hex_grid, schools_sf))
```

## Summary Statistics

```{r}
summary(hex_grid$school_COUNT)
```
:::

From the above, we noticed that there are excessive 0 values in the field of each variables. Notably, we would still transform this field with `log()`. Thus, additional step is required to ensure that the 0 will be replaced with a value that is between 0 and 1.

# 4. Distance Matrix

Before we continue with the previous section, we will compute a distance matrix. It is a table that shows the distance between pairs of locations. In our context, we want to see the distance between one hexagonal grid to another.

## 4.1 Converting from sf data.table to SpatialPolygons Data Frame

To begin, we will use `sp::as.Spatial()` to convert **hex_grid** from sf tabble data frame to SpatialPolygonsDataFrame of sp object.

```{r}
hex_grid_sp <- as(hex_grid, "Spatial")
hex_grid_sp
```

## 4.2 Computing the distance matrix

Next, we compute the Euclidean distance between the centroids of the hexagonal grid with the use of `spDists()` of **sp** package. `head()` is also used to examine a 10x10 matrix.

```{r}
#| code-fold: true
dist <- spDists(hex_grid_sp, 
                longlat = FALSE)
head(dist, n=c(10, 10))
```

From the matrix, we noted that the headers for the columns and row are not labelled . Therefore, we will add a label into the distance matrix.

### 4.2.1 Label of Column and Row headers

To tidy, we will create a list sorted according to the distance matrix by `grid_id`.

```{r}
id_names <- hex_grid$grid_id
```

Next, we will attach `grid_id` to row and column for distance matrix matching.

```{r}
colnames(dist) <- paste0(id_names)
rownames(dist) <- paste0(id_names)
```

### 4.2.2 Pivoting distance value by Grid_ID

Then, we will pivot the distance matrix into a long table by using the row and column of grid_id.

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

From the data frame above, we noticed that the intra-zonal distance within the zone is zero. As such, we will append a constant value to replace the intra-zonal distance of 0 by performing the following:

-   Find out the minimum value of the distance by using `summary()`
-   Input a constant distance value
-   Rename original and destination fields

::: panel-tabset
# Minimum Value

```{r}
#| code-fold: true
# find minimum distance value
distPair %>%
  filter(dist > 0) %>%
  summary()
```

# Derive Constant Distance Value

```{r}
#| code-fold: true
distPair$dist <- ifelse(distPair$dist == 0,
                        200, distPair$dist)

# to check the distPair data frame
distPair %>%
  summary()

```

# Rename and Save RDS

```{r}
#to rename the fields
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)

#save output to rds format. 
write_rds(distPair, "data/rds/distPair.rds") 

```
:::

We determined that the the minimum distance is 750m. Although we could put any values that are smaller than **750m** to represent the intra-zonal distance, we input an arbitrary value of **200m** that is smaller than the radius of the hexagon. We will save a copy of **distPair** output to RDS and re-load it into the R environment.

```{r}
distPair <- read_rds("data/rds/distPair.rds")
```

## 4.3 Prepare flow data

To get the flow data, we will compute the flow movement between and within the hexagonal grid by using the code chunk below. Output will be saved as **flow_data**. In addition, we will use `head()` to display the top 10 data frame.

```{r}
#| code-fold: true
flow_data <- od_data1 %>%
  group_by(ORIGIN_ID, DESTIN_ID) %>% 
  summarize(TRIPS = sum(MORNING_PEAK)) 


head(flow_data,10)
```

### 4.3.1 Separate Intra-flow

We will use `ifelse()` function to add three new fields in the dataframe with the condition that if Origin ID match Destination ID , zero will be forced.

```{r}
#| code-fold: true
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_ID == flow_data$DESTIN_ID, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_ID == flow_data$DESTIN_ID, 
  0.000001, 1)

glimpse(flow_data)
```

### 4.3.2 Combine flow_data and distPair

Before we join the data, we have to ensure that the data type is identical. As seen above, the flow_data for the grids are in `<fct>` format. Therefore, we will look at **distPai**r.

```{r}
glimpse(distPair)
```

From the results above, we note that the `orig` and `dest` are in `<int>` format. Similar, to section 2.3, we need to convert the data type from `<int>` to `<fct>`. After converting, we will run `glimpse()` again to check .

```{r}
#| code-fold: true
#convert distPair from int to fct 
distPair$orig <- as.factor(distPair$orig)
distPair$dest <- as.factor(distPair$dest)

glimpse(distPair)
```

Now, we are ready to perform a `dplyr::left_join()` to *flow_data* dataframe and *distPair* dataframe. The output will be saved as **flow_data1**.

```{r}
flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("ORIGIN_ID" = "orig",
                    "DESTIN_ID" = "dest"))
```

## 4.4 Preparing Origin and Destination Attributes

Next, we will add the origin and destination attributes by joinining it to the **hex_grid** data. We will process in two steps:

-   Add Origin attrbitues through flow_data using hex_grid by performing `left_join()`

-   Using the previous result, perform another `left_join()` to add destination attributes.

In addition, we tidy the data by renaming columns to include a prefix. Origin attributes will be re-labeled as **ORIGIN_XX,** while destination attributes will be re-labeled as **DESTIN_XX**. As we do not need the bus_stop count and geometry, we will drop the columns.

```{r}
#| code-fold: true
#left join based on origin attributes 
flow_data2 <- flow_data1 %>%
  left_join(hex_grid,
            by = c(ORIGIN_ID = "grid_id")) %>%
    rename(ORIGIN_BUSINESS = BUSINESS_COUNT,
         ORIGIN_FINSER = finserv_COUNT,
         ORIGIN_HDB = HDB_SUM,
         ORIGIN_RETAIL = retail_COUNT,
         ORIGIN_SCHOOLS =school_COUNT
         ) %>%
  select(-c(bus_stops)) 

#left join based on destination attributes
flow_data2 <- flow_data2 %>%
  left_join(hex_grid,
            by = c(DESTIN_ID = "grid_id")) %>%
    rename(DESTIN_BUSINESS = BUSINESS_COUNT,
         DESTIN_FINSER = finserv_COUNT,
         DESTIN_HDB = HDB_SUM,
         DESTIN_RETAIL = retail_COUNT,
         DESTIN_SCHOOLS =school_COUNT
         ) %>%
  select(-c(bus_stops)) %>%
  select(-c(10,16))

```

# 5. Calibrating Spatial Interaction Models

Spatial interaction describe quatitatively the flow of people, material, or information between locations in geographical space.In this section, we will be looking at gravity models using the `glm()` function in R, namely:

-   Unconstrained Spatial Interaction Model

-   Origin Constrained Spatial Interaction Model

-   Destination Constrained Spatial Interaction Model

-   Doubly Constrained Spatial Interaction Model

## 5.1 Visualizing dependent variable

Before we run our models, we will do a quick visualization to visualize the distribution, and the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely *distance*.

```{r}
#| code-fold: true
ggplot(data = flow_data2,
       aes(x = TRIPS)) +
  geom_histogram() +
  labs(title ="Distribution of Dependent Variable") +
  theme_minimal()
```

From the graph above, we noticed that the distribution is highly skewed and does not resembled a normal distribution. We will move on to visualize the relation.

```{r}
#| code-fold: true
ggplot(data = flow_data2,
       aes(x = dist,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm) +
  labs(title ="Relation between Dependent and Independent Variable") +
  theme_minimal()
```

Based on the visualization above, it is difficult to determine if it resemble linear relationship. Therefore, we will use the log transformed version of both the dependent and independent variables, to see if they have a linear relationship.

```{r}
#| code-fold: true
ggplot(data = flow_data2,
       aes(x = log(dist),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm) +
  labs(title ="Relation between Dependent and Independent Variable (log)") +
  theme_minimal()
```

From the graph above, we are to visualize a better linear relationship in comparison to the previous scatterplot.

## 5.2 Checking for variables with zero values

Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure to ensure that our exploratory variables does not contain any 0 values We will used the `summary()` of Base R to compute the summary statistics of all variables in the **flow_data2** data frame.

```{r}
summary(flow_data2)
```

From the summary statistics, we confirmed that we have removed intra-flow as the min of **FlowNoIntra** is 1. However, all our variables - `ORIGIN_BUSINESS`, `ORIGIN_FINSER`, **`ORIGIN_HDB`**, `ORIGIN_RETAIL`, `ORIGIN_SCHOOLS`, `DESTIN_BUSINESS`, `DESTIN_FINSER`, `DESTIN_HDB`, `DESTIN_RETAIL`, `DESTIN_SCHOOLS` consinst of 0 values.

As such, we will use the code chunk below to replace it with 0.99.

```{r}
#| code-fold: true

#ORIGIN_BUSINESS
flow_data2$ORIGIN_BUSINESS <- ifelse(
  flow_data2$ORIGIN_BUSINESS == 0,
  0.99, flow_data2$ORIGIN_BUSINESS)

#ORIGIN_FINSER 
flow_data2$ORIGIN_FINSER  <- ifelse(
  flow_data2$ORIGIN_FINSER == 0,
  0.99, flow_data2$ORIGIN_FINSER)

#ORIGIN_HDB
flow_data2$ORIGIN_HDB  <- ifelse(
  flow_data2$ORIGIN_HDB == 0,
  0.99, flow_data2$ORIGIN_HDB)

#ORIGIN_RETAIL
flow_data2$ORIGIN_RETAIL  <- ifelse(
  flow_data2$ORIGIN_RETAIL == 0,
  0.99, flow_data2$ORIGIN_RETAIL)

#ORIGIN_SCHOOLS
flow_data2$ORIGIN_SCHOOLS  <- ifelse(
  flow_data2$ORIGIN_SCHOOLS == 0,
  0.99, flow_data2$ORIGIN_SCHOOLS)

#DESTIN_BUSINESS
flow_data2$DESTIN_BUSINESS <- ifelse(
  flow_data2$DESTIN_BUSINESS == 0,
  0.99, flow_data2$DESTIN_BUSINESS)

#DESTIN_FINSER 
flow_data2$DESTIN_FINSER  <- ifelse(
  flow_data2$DESTIN_FINSER == 0,
  0.99, flow_data2$DESTIN_FINSER)

#DESTIN_HDB
flow_data2$DESTIN_HDB  <- ifelse(
  flow_data2$DESTIN_HDB == 0,
  0.99, flow_data2$DESTIN_HDB)

#DESTIN_RETAIL
flow_data2$DESTIN_RETAIL  <- ifelse(
  flow_data2$DESTIN_RETAIL == 0,
  0.99, flow_data2$DESTIN_RETAIL)

#DESTIN_SCHOOLS
flow_data2$DESTIN_SCHOOLS  <- ifelse(
  flow_data2$DESTIN_SCHOOLS == 0,
  0.99, flow_data2$DESTIN_SCHOOLS)
```

To confirm , we re-run the `summary()` before saving the output in RDS format and re-load it into R.

```{r}
summary(flow_data2)
```

::: {.callout-note title="Save and Load RDS" collapse="true" appearance="simple" icon="false"}
## Save and Load RDS

```{r}
write_rds(flow_data2, "data/rds/flow_data2.rds")
flow_data2 <- read_rds("data/rds/flow_data2.rds")
```
:::

## 5.3 Unconstrained Spatial Interaction Model

The first model that we will looking at will be the Unconstrained model. There are no external restrictions on the flow value. Thus, any origin can send flows to any destination based solely on the attraction potential of the destination and the distance. We will save the output - **uncSIM** into a RDS format and re-load into the R environment.

```{r}
#| code-fold: true
#| eval: false
uncSIM <- glm(formula = TRIPS ~ 
                log(ORIGIN_BUSINESS) + 
                log(DESTIN_BUSINESS) +
                log(ORIGIN_FINSER) + 
                log(DESTIN_FINSER) +
                log(ORIGIN_HDB) + 
                log(DESTIN_HDB) +
                log(ORIGIN_RETAIL) + 
                log(DESTIN_RETAIL) +
                log(ORIGIN_SCHOOLS) + 
                log(DESTIN_SCHOOLS) +
                log(dist),
              family = poisson(link = "log"),
              data = flow_data2,
              na.action = na.exclude)
#save RDS
write_rds(uncSIM, "data/rds/uncSIM.rds")
```

::: {.callout-note title="Load RDS" collapse="true" appearance="simple" icon="false"}
## Load uncSIM RDS

```{r}
uncSIM <- read_rds("data/rds/uncSIM.rds")
```
:::

## View uncSIM

```{r}
uncSIM
```

## Calculate R-squared

```{r}
# R-squared function

#create function to calculate r-squared value 
CalcRSquared <- function(observed,estimated){
  r <- cor(observed,estimated)
  R2 <- r^2
  R2
}
```

```{r}
CalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)
```

```{r}
r2_mcfadden(uncSIM)
```

## 5.4 Origin (Production) constrained SIM

The second model that we are looking at will be the Origin Constrained model. The flow value are based on the destination attributes. We will save the output - **orcSIM** into a RDS format and re-load into the R environment.

```{r}
#| code-fold: true
#| eval: false
orcSIM <- glm(formula = TRIPS ~ 
                 ORIGIN_ID +
                 log(DESTIN_BUSINESS) +
                 log(DESTIN_FINSER) +
                 log(DESTIN_HDB) +
                 log(DESTIN_RETAIL) +
                 log(DESTIN_SCHOOLS) +
                 log(dist),
              family = poisson(link = "log"),
              data = flow_data2,
              na.action = na.exclude)
#save RDS
write_rds(orcSIM, "data/rds/orcSIM.rds")

```

::: {.callout-note title="Load RDS" collapse="true" appearance="simple"}
5.5

## Load orcSIM RDS

```{r}
orcSIM <- read_rds("data/rds/orcSIM.rds")
```
:::

::: {.callout-tip title="Summary(orcSIM)" collapse="true" appearance="simple" icon="false"}
## Summary(orcSIM)

```{r}
summary(orcSIM)
options(max.print = 1000000)
```
:::

## 5.5 Destination constrained SIM

The third model that we are looking at will be the Destination Constrained model. The flow value are based on the origin attributes. We will save the output - **decSIM** into a RDS format and re-load into the R environment.

```{r}
#| code-fold: true
#| eval: false
decSIM <- glm(formula = TRIPS ~ 
                DESTIN_ID + 
                log(ORIGIN_BUSINESS) + 
                log(ORIGIN_FINSER) + 
                log(ORIGIN_HDB) + 
                log(ORIGIN_RETAIL) + 
                log(ORIGIN_SCHOOLS) + 
                log(dist),
              family = poisson(link = "log"),
              data = flow_data2,
              na.action = na.exclude)
#save RDS
write_rds(decSIM, "data/rds/decSIM.rds")
```

::: {.callout-note title="Load RDS" collapse="true" appearance="simple"}
## Load decSIM RDS

```{r}
decSIM <- read_rds("data/rds/decSIM.rds")
```
:::

::: {.callout-tip title="Summary(decSIM)" collapse="true" appearance="simple" icon="false"}
## Summary(decSIM)

```{r}
summary(decSIM)
```
:::

## 5.6 Doubly constrained SIM

The last model that we are looking at will be the Doubly Constrained model. The flow value are not pre-determined. We will save the output - **dbcSIM** into a RDS format and re-load into the R environment.

```{r}
#| code-fold: true
#| eval: false
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_ID + 
                DESTIN_ID + 
                log(dist),
              family = poisson(link = "log"),
              data = flow_data2,
              na.action = na.exclude)
#save RDS
write_rds(dbcSIM, "data/rds/dbcSIM.rds")
```

::: {.callout-note title="Load RDS" collapse="true" appearance="simple"}
## Load dbcSIM RDS

```{r}
dbcSIM <- read_rds("data/rds/dbcSIM.rds")
```
:::

::: {.callout-tip title="Summary(dbcSIM)" collapse="true" appearance="simple" icon="false"}
## Summary(dbcSIM)

```{r}
summary(dbcSIM)

```
:::

## Calculate R-squared

```{r}
CalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)
```

## 5.7 Model Comparison

```{r}
model_list <- list(unconstrained=uncSIM,
                   originConstrained=orcSIM,
                   destinationConstrained=decSIM,
                   doublyConstrained=dbcSIM)

```

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

# Visualize Fitting

```{r}
df <- as.data.frame(uncSIM$fitted.values) %>%
  round(digits = 0)

flow_data2 <- flow_data2 %>%
  cbind(df) %>%
  rename(uncTRIPS = "uncSIM$fitted.values")
```

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)

flow_data2 <- flow_data2 %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM$fitted.values")
```

```{r}
df <- as.data.frame(decSIM$fitted.values) %>%
  round(digits = 0)

flow_data2 <- flow_data2 %>%
  cbind(df) %>%
  rename(decTRIPS = "decSIM$fitted.values")
```

```{r}
df <- as.data.frame(dbcSIM$fitted.values) %>%
  round(digits = 0)

flow_data2 <- flow_data2 %>%
  cbind(df) %>%
  rename(dbcTRIPS = "dbcSIM$fitted.values")

```

```{r}
unc_p <- ggplot(data = flow_data2,
                aes(x = uncTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

orc_p <- ggplot(data = flow_data2,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dec_p <- ggplot(data = flow_data2,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

dbc_p <- ggplot(data = flow_data2,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

ggarrange(unc_p, orc_p, dec_p, dbc_p,
          ncol = 2,
          nrow = 2)
```
